from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.llms.ollama import Ollama  # Ollamaìš© LLM
import gradio as gr

# Ollama LLM ì„¤ì •
llm = Ollama(model="mistral")  # ì‚¬ìš©í•˜ë ¤ëŠ” Ollama ëª¨ë¸ ì§€ì •

def answer(message, history):
    files = []
    for msg in history:
        if msg['role'] == "user" and isinstance(msg['content'], tuple):
            files.append(msg['content'][0])
    for file in message["files"]:
        files.append(file)

    # íŒŒì¼ì´ ì—†ìœ¼ë©´ ì˜¤ë¥˜ ë°©ì§€
    if not files:
        return "ğŸ“‚ ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ ë˜ëŠ” PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”."

    # íŒŒì¼ ë¡œë“œ ë° RAG ì¸ë±ì‹±
    try:
        documents = SimpleDirectoryReader(input_files=files).load_data()
        index = VectorStoreIndex.from_documents(documents)
        query_engine = index.as_query_engine(llm=llm)
        return str(query_engine.query(message["text"]))
    except Exception as e:
        return f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

demo = gr.ChatInterface(
    answer,
    type="messages",
    title="Llama Index RAG Chatbot (Ollama)",
    description="Upload any text or PDF files and ask questions about them!",
    textbox=gr.MultimodalTextbox(file_types=[".pdf", ".txt"]),
    multimodal=True
)

demo.launch()
