import gradio as gr
import lancedb
from llama_index.vector_stores.lancedb import LanceDBVectorStore
from llama_index.llms.ollama import Ollama
from llama_index.core import VectorStoreIndex, StorageContext, SimpleDirectoryReader
import os
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core import Settings
import pandas as pd
from llama_index.core import ServiceContext


# Settingsë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”© ë° LLM ì„¤ì •
Settings.embed_model = HuggingFaceEmbedding(model_name="sentence-transformers/all-mpnet-base-v2")
#Settings.embed_model = HuggingFaceEmbedding(model_name="bert-base-uncased")  # 768 ì°¨ì›ì˜ ë²¡í„°ë¥¼ ìƒì„±í•˜ëŠ” ëª¨ë¸
#Settings.embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")
#Settings.embed_model = HuggingFaceEmbedding(model_name="gemma2")

# ğŸ“Œ Ollama ëª¨ë¸ ì„¤ì • (ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©)
#llm = OllamaLLM(model_name="gemma2")  # Ollama ëª¨ë¸ ì„¤ì •
#Settings.llm = llm


llm = Ollama(model="gemma2",base_url="http://localhost:11434/v1")
Settings.llm = llm

# ğŸ“Œ GraphRAGì—ì„œ ìƒì„±í•œ LanceDB ë¶ˆëŸ¬ì˜¤ê¸°
db_path = "/Users/jun/GitStudy/Data_4/Data/project5/model/graphrag_t_2/output/lancedb"
db = lancedb.connect(db_path)  # GraphRAGì—ì„œ ì‚¬ìš©í•œ DB ê²½ë¡œ 

# âœ… í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸ í›„ ë¶ˆëŸ¬ì˜¤ê¸°
table_names = db.table_names()
# âœ… ëª¨ë“  í…Œì´ë¸”ì„ ë¶ˆëŸ¬ì™€ì„œ VectorStoreIndex ìƒì„±
vector_stores = {name: LanceDBVectorStore(table=db.open_table(name)) for name in table_names}
storage_contexts = {name: StorageContext.from_defaults(vector_store=store) for name, store in vector_stores.items()}
indexes = {name: VectorStoreIndex.from_vector_store(store, storage_context=storage_contexts[name]) for name, store in vector_stores.items()}


# âœ… ì—¬ëŸ¬ í…Œì´ë¸”ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ Query ì—”ì§„ì„ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì €ì¥
query_engines = {name: index.as_query_engine() for name, index in indexes.items()}

# ğŸ“Œ íŒŒì¼ ì—…ë¡œë“œ í›„ ë¬¸ì„œ ì²˜ë¦¬
def process_uploaded_files(files):
    """ì‚¬ìš©ìê°€ ì—…ë¡œë“œí•œ íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ LlamaIndexì— ì¶”ê°€"""
    if not files:
        return None  # íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¬´ì‹œ

    # íŒŒì¼ ì €ì¥ ê²½ë¡œ
    upload_dir = "uploaded_files"
    os.makedirs(upload_dir, exist_ok=True)

    # ì—…ë¡œë“œëœ íŒŒì¼ ì €ì¥
    file_paths = []
    for file in files:
        file_path = os.path.join(upload_dir, file.name)
        file_paths.append(file_path)

        # Parquet íŒŒì¼ ì²˜ë¦¬
        if file.name.endswith(".parquet"):
            try:
                df = pd.read_parquet(file)  # âœ… Parquet íŒŒì¼ì„ DataFrameìœ¼ë¡œ ë³€í™˜
                text_data = df.to_string(index=False)  # âœ… DataFrameì„ ë¬¸ìì—´ë¡œ ë³€í™˜
                text_file_path = file_path.replace(".parquet", ".txt")  # ë³€í™˜ëœ íŒŒì¼ëª…
                with open(text_file_path, "w", encoding="utf-8") as text_file:
                    text_file.write(text_data)  # âœ… TXT íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ LlamaIndexê°€ ì½ì„ ìˆ˜ ìˆë„ë¡ ë³€í™˜
                file_paths.append(text_file_path)  # ë³€í™˜ëœ íŒŒì¼ì„ ì¶”ê°€
            except Exception as e:
                print(f"âŒ Parquet íŒŒì¼ ë³€í™˜ ì‹¤íŒ¨: {e}")
                return None  # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ

        else:
            # ê¸°ì¡´ ë°©ì‹ëŒ€ë¡œ íŒŒì¼ ì €ì¥
            with open(file_path, "wb") as f:
                f.write(file.read())

    # ğŸ“Œ ìƒˆ ë¬¸ì„œ ì¸ë±ì‹±
    documents = SimpleDirectoryReader(input_files=file_paths).load_data()
    new_index = VectorStoreIndex.from_documents(documents)
    return new_index.as_query_engine()

# ğŸ“Œ ì‚¬ìš©ì ë©”ì‹œì§€ ì²˜ë¦¬
def answer(message, history, files):
    
    global query_engines
    """ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë°›ê³ , ê¸°ì¡´ GraphRAG ë°ì´í„° + ì—…ë¡œë“œëœ ë¬¸ì„œ ë°ì´í„°ë¡œ ë‹µë³€"""
    
    # ê¸°ì¡´ GraphRAG ë°ì´í„° (ì—¬ëŸ¬ í…Œì´ë¸”ì—ì„œ ì²˜ë¦¬)
    query_engine_list = list(query_engines.values())  
    
    # ì—…ë¡œë“œëœ íŒŒì¼ì´ ìˆì„ ê²½ìš° ìƒˆë¡­ê²Œ ì¸ë±ì‹±í•˜ì—¬ ì¶”ê°€
    new_query_engine = process_uploaded_files(files)
    if new_query_engine:
        query_engine_list.append(new_query_engine)
        query_engines["uploaded_files"] = new_query_engine # âœ… ì—…ë¡œë“œí•œ ë¬¸ì„œë„ ì¶”ê°€
    
    # ëª¨ë“  ì¿¼ë¦¬ ì—”ì§„ì—ì„œ ì§ˆì˜ ìˆ˜í–‰
    responses = []
    for qe in query_engine_list:
        if hasattr(qe, 'query'):  # query ì—”ì§„ì´ query ë©”ì„œë“œë¥¼ ê°€ì§€ê³  ìˆëŠ”ì§€ í™•ì¸
            responses.append(qe.query(message["text"]))
        else:
            print(f"âŒ {qe}ëŠ” query ë©”ì„œë“œë¥¼ ê°€ì§€ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    # ğŸ“Œ ì‘ë‹µì„ ì¢…í•©í•˜ì—¬ ë°˜í™˜
    return "\n\n---\n\n".join([str(resp) for resp in responses])

# ğŸ“Œ Gradio ì¸í„°í˜ì´ìŠ¤ ì„¤ì •
demo = gr.ChatInterface(
    answer,
    type="messages",
    title="GraphRAG + Ollama RAG Chatbot",
    description="GraphRAGì—ì„œ ìƒì„±í•œ LanceDB ë°ì´í„°ì™€ ì‚¬ìš©ìê°€ ì—…ë¡œë“œí•œ ë¬¸ì„œë¥¼ í™œìš©í•œ Ollama ê¸°ë°˜ RAG Chatbot!",
    textbox=gr.MultimodalTextbox(file_types=[".pdf", ".txt"]),
    multimodal=True  # íŒŒì¼ ì—…ë¡œë“œ í—ˆìš©
)

# ğŸ“Œ ì‹¤í–‰
demo.launch()