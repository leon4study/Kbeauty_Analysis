{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import lancedb\n",
    "from llama_index.vector_stores.lancedb import LanceDBVectorStore\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import VectorStoreIndex, StorageContext, SimpleDirectoryReader\n",
    "import os\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "import pandas as pd\n",
    "from llama_index.core import ServiceContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settingsë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”© ë° LLM ì„¤ì •\n",
    "#Settings.embed_model = HuggingFaceEmbedding(model_name=\"bert-base-uncased\")  # 768 ì°¨ì›ì˜ ë²¡í„°ë¥¼ ìƒì„±í•˜ëŠ” ëª¨ë¸\n",
    "#Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "#all-mpnet-base-v2 (768ì°¨ì›, ì •í™•ë„ ë†’ìŒ, ì†ë„ ì¤‘ê°„)\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "\n",
    "#Settings.embed_model = HuggingFaceEmbedding(model_name=\"gemma2\")\n",
    "\n",
    "# ğŸ“Œ Ollama ëª¨ë¸ ì„¤ì • (ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©)\n",
    "#llm = OllamaLLM(model_name=\"gemma2\")  # Ollama ëª¨ë¸ ì„¤ì •\n",
    "#Settings.llm = llm\n",
    "\n",
    "\n",
    "llm = Ollama(model=\"gemma2\",base_url=\"http://localhost:11434/v1\")\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanceDBConnection(/Users/jun/GitStudy/Data_4/Data/project5/model/graphrag_t_2/output/lancedb)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ğŸ“Œ GraphRAGì—ì„œ ìƒì„±í•œ LanceDB ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "db_path = \"/Users/jun/GitStudy/Data_4/Data/project5/model/graphrag_t_2/output/lancedb\"\n",
    "db = lancedb.connect(db_path)  # GraphRAGì—ì„œ ì‚¬ìš©í•œ DB ê²½ë¡œ \n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>vector</th>\n",
       "      <th>attributes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0985dfb0576a493a90740467e97a46fd</td>\n",
       "      <td># ADVANCED SNAIL 96 MUCIN POWER ESSENCE and RE...</td>\n",
       "      <td>[0.017612401, 0.0761737, -0.17061758, -0.07786...</td>\n",
       "      <td>{\"title\": \"# ADVANCED SNAIL 96 MUCIN POWER ESS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  \\\n",
       "0  0985dfb0576a493a90740467e97a46fd   \n",
       "\n",
       "                                                text  \\\n",
       "0  # ADVANCED SNAIL 96 MUCIN POWER ESSENCE and RE...   \n",
       "\n",
       "                                              vector  \\\n",
       "0  [0.017612401, 0.0761737, -0.17061758, -0.07786...   \n",
       "\n",
       "                                          attributes  \n",
       "0  {\"title\": \"# ADVANCED SNAIL 96 MUCIN POWER ESS...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.open_table(\"default-community-full_content\").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'default-community-full_content': LanceDBVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=True, uri='/tmp/lancedb', vector_column_name='vector', nprobes=20, refine_factor=None, text_key='text', doc_id_key='doc_id', api_key=None, region=None, mode='overwrite', query_type='vector', overfetch_factor=1),\n",
       " 'default-entity-description': LanceDBVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=True, uri='/tmp/lancedb', vector_column_name='vector', nprobes=20, refine_factor=None, text_key='text', doc_id_key='doc_id', api_key=None, region=None, mode='overwrite', query_type='vector', overfetch_factor=1),\n",
       " 'default-text_unit-text': LanceDBVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=True, uri='/tmp/lancedb', vector_column_name='vector', nprobes=20, refine_factor=None, text_key='text', doc_id_key='doc_id', api_key=None, region=None, mode='overwrite', query_type='vector', overfetch_factor=1)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# âœ… í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸ í›„ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "table_names = db.table_names()\n",
    "# âœ… ëª¨ë“  í…Œì´ë¸”ì„ ë¶ˆëŸ¬ì™€ì„œ VectorStoreIndex ìƒì„±\n",
    "vector_stores = {name: LanceDBVectorStore(table=db.open_table(name)) for name in table_names}\n",
    "vector_stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'default-community-full_content': StorageContext(docstore=<llama_index.core.storage.docstore.simple_docstore.SimpleDocumentStore object at 0x316b36750>, index_store=<llama_index.core.storage.index_store.simple_index_store.SimpleIndexStore object at 0x344105010>, vector_stores={'default': LanceDBVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=True, uri='/tmp/lancedb', vector_column_name='vector', nprobes=20, refine_factor=None, text_key='text', doc_id_key='doc_id', api_key=None, region=None, mode='overwrite', query_type='vector', overfetch_factor=1), 'image': SimpleVectorStore(stores_text=False, is_embedding_query=True, data=SimpleVectorStoreData(embedding_dict={}, text_id_to_ref_doc_id={}, metadata_dict={}))}, graph_store=<llama_index.core.graph_stores.simple.SimpleGraphStore object at 0x3440fbdd0>, property_graph_store=None),\n",
       " 'default-entity-description': StorageContext(docstore=<llama_index.core.storage.docstore.simple_docstore.SimpleDocumentStore object at 0x3440fb990>, index_store=<llama_index.core.storage.index_store.simple_index_store.SimpleIndexStore object at 0x3440fb510>, vector_stores={'default': LanceDBVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=True, uri='/tmp/lancedb', vector_column_name='vector', nprobes=20, refine_factor=None, text_key='text', doc_id_key='doc_id', api_key=None, region=None, mode='overwrite', query_type='vector', overfetch_factor=1), 'image': SimpleVectorStore(stores_text=False, is_embedding_query=True, data=SimpleVectorStoreData(embedding_dict={}, text_id_to_ref_doc_id={}, metadata_dict={}))}, graph_store=<llama_index.core.graph_stores.simple.SimpleGraphStore object at 0x3440fb210>, property_graph_store=None),\n",
       " 'default-text_unit-text': StorageContext(docstore=<llama_index.core.storage.docstore.simple_docstore.SimpleDocumentStore object at 0x3440fb010>, index_store=<llama_index.core.storage.index_store.simple_index_store.SimpleIndexStore object at 0x3440fac10>, vector_stores={'default': LanceDBVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=True, uri='/tmp/lancedb', vector_column_name='vector', nprobes=20, refine_factor=None, text_key='text', doc_id_key='doc_id', api_key=None, region=None, mode='overwrite', query_type='vector', overfetch_factor=1), 'image': SimpleVectorStore(stores_text=False, is_embedding_query=True, data=SimpleVectorStoreData(embedding_dict={}, text_id_to_ref_doc_id={}, metadata_dict={}))}, graph_store=<llama_index.core.graph_stores.simple.SimpleGraphStore object at 0x3440fa950>, property_graph_store=None)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storage_contexts = {name: StorageContext.from_defaults(vector_store=store) for name, store in vector_stores.items()}\n",
    "storage_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'default-community-full_content': <llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x34410d990>,\n",
       " 'default-entity-description': <llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x3440cee50>,\n",
       " 'default-text_unit-text': <llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x3440cd310>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes = {name: VectorStoreIndex.from_vector_store(store, storage_context=storage_contexts[name]) for name, store in vector_stores.items()}\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'default-community-full_content': <llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine at 0x341f87210>,\n",
       " 'default-entity-description': <llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine at 0x3440c5890>,\n",
       " 'default-text_unit-text': <llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine at 0x3440c56d0>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# âœ… ì—¬ëŸ¬ í…Œì´ë¸”ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ Query ì—”ì§„ì„ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì €ì¥\n",
    "query_engines = {name: index.as_query_engine() for name, index in indexes.items()}\n",
    "query_engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Œ íŒŒì¼ ì—…ë¡œë“œ í›„ ë¬¸ì„œ ì²˜ë¦¬\n",
    "def process_uploaded_files(files):\n",
    "    \"\"\"ì‚¬ìš©ìê°€ ì—…ë¡œë“œí•œ íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ LlamaIndexì— ì¶”ê°€\"\"\"\n",
    "    if not files:\n",
    "        return None  # íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¬´ì‹œ\n",
    "\n",
    "    # íŒŒì¼ ì €ì¥ ê²½ë¡œ\n",
    "    upload_dir = \"uploaded_files\"\n",
    "    os.makedirs(upload_dir, exist_ok=True)\n",
    "\n",
    "    # ì—…ë¡œë“œëœ íŒŒì¼ ì €ì¥\n",
    "    file_paths = []\n",
    "    for file in files:\n",
    "        file_path = os.path.join(upload_dir, file.name)\n",
    "        file_paths.append(file_path)\n",
    "\n",
    "        # Parquet íŒŒì¼ ì²˜ë¦¬\n",
    "        if file.name.endswith(\".parquet\"):\n",
    "            try:\n",
    "                df = pd.read_parquet(file)  # âœ… Parquet íŒŒì¼ì„ DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "                text_data = df.to_string(index=False)  # âœ… DataFrameì„ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "                text_file_path = file_path.replace(\".parquet\", \".txt\")  # ë³€í™˜ëœ íŒŒì¼ëª…\n",
    "                with open(text_file_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "                    text_file.write(text_data)  # âœ… TXT íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ LlamaIndexê°€ ì½ì„ ìˆ˜ ìˆë„ë¡ ë³€í™˜\n",
    "                file_paths.append(text_file_path)  # ë³€í™˜ëœ íŒŒì¼ì„ ì¶”ê°€\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Parquet íŒŒì¼ ë³€í™˜ ì‹¤íŒ¨: {e}\")\n",
    "                return None  # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ\n",
    "\n",
    "        else:\n",
    "            # ê¸°ì¡´ ë°©ì‹ëŒ€ë¡œ íŒŒì¼ ì €ì¥\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                f.write(file.read())\n",
    "\n",
    "    # ğŸ“Œ ìƒˆ ë¬¸ì„œ ì¸ë±ì‹±\n",
    "    documents = SimpleDirectoryReader(input_files=file_paths).load_data()\n",
    "    new_index = VectorStoreIndex.from_documents(documents)\n",
    "    return new_index.as_query_engine()\n",
    "\n",
    "# ğŸ“Œ ì‚¬ìš©ì ë©”ì‹œì§€ ì²˜ë¦¬\n",
    "def answer(message, history, files):\n",
    "    \n",
    "    global query_engines\n",
    "    \"\"\"ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë°›ê³ , ê¸°ì¡´ GraphRAG ë°ì´í„° + ì—…ë¡œë“œëœ ë¬¸ì„œ ë°ì´í„°ë¡œ ë‹µë³€\"\"\"\n",
    "    \n",
    "    # ê¸°ì¡´ GraphRAG ë°ì´í„° (ì—¬ëŸ¬ í…Œì´ë¸”ì—ì„œ ì²˜ë¦¬)\n",
    "    query_engine_list = list(query_engines.values())  \n",
    "    \n",
    "    # ì—…ë¡œë“œëœ íŒŒì¼ì´ ìˆì„ ê²½ìš° ìƒˆë¡­ê²Œ ì¸ë±ì‹±í•˜ì—¬ ì¶”ê°€\n",
    "    new_query_engine = process_uploaded_files(files)\n",
    "    if new_query_engine:\n",
    "        query_engine_list.append(new_query_engine)\n",
    "        query_engines[\"uploaded_files\"] = new_query_engine # âœ… ì—…ë¡œë“œí•œ ë¬¸ì„œë„ ì¶”ê°€\n",
    "    \n",
    "    # ëª¨ë“  ì¿¼ë¦¬ ì—”ì§„ì—ì„œ ì§ˆì˜ ìˆ˜í–‰\n",
    "    responses = []\n",
    "    for qe in query_engine_list:\n",
    "        print(qe)\n",
    "        if hasattr(qe, 'query'):  # query ì—”ì§„ì´ query ë©”ì„œë“œë¥¼ ê°€ì§€ê³  ìˆëŠ”ì§€ í™•ì¸\n",
    "            responses.append(qe.query(message[\"text\"]))\n",
    "        else:\n",
    "            print(f\"âŒ {qe}ëŠ” query ë©”ì„œë“œë¥¼ ê°€ì§€ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # ğŸ“Œ ì‘ë‹µì„ ì¢…í•©í•˜ì—¬ ë°˜í™˜\n",
    "    return \"\\n\\n---\\n\\n\".join([str(resp) for resp in responses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/gradio/helpers.py:968: UserWarning: Unexpected argument. Filling with None.\n",
      "  warnings.warn(\"Unexpected argument. Filling with None.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine object at 0x341f87210>\n",
      "relation :  {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/llama_index/vector_stores/lancedb/base.py\", line 529, in query\n",
      "    print(f\"item_json : {item_json}\")\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/llama_index/core/vector_stores/utils.py\", line 70, in metadata_dict_to_node\n",
      "    raise ValueError(\"Node content not found in metadata dict.\")\n",
      "ValueError: Node content not found in metadata dict.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 3805, in get_loc\n",
      "    return self._engine.get_loc(casted_key)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"index.pyx\", line 167, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"index.pyx\", line 196, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7081, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7089, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 'doc_id'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/gradio/blocks.py\", line 2051, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/gradio/blocks.py\", line 1596, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/gradio/utils.py\", line 850, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/gradio/chat_interface.py\", line 861, in _submit_fn\n",
      "    response = await anyio.to_thread.run_sync(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2441, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 943, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/29/5wmqbm4j3x10qv5vgm_bm8f80000gn/T/ipykernel_69986/2332650835.py\", line 60, in answer\n",
      "    responses.append(qe.query(message[\"text\"]))\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 321, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py\", line 52, in query\n",
      "    query_result = self._query(str_or_query_bundle)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 321, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py\", line 178, in _query\n",
      "    nodes = self.retrieve(query_bundle)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py\", line 133, in retrieve\n",
      "    nodes = self._retriever.retrieve(query_bundle)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 321, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/llama_index/core/base/base_retriever.py\", line 245, in retrieve\n",
      "    nodes = self._retrieve(query_bundle)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 321, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/llama_index/core/indices/vector_store/retrievers/retriever.py\", line 103, in _retrieve\n",
      "    return self._get_nodes_with_embeddings(query_bundle)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/llama_index/core/indices/vector_store/retrievers/retriever.py\", line 180, in _get_nodes_with_embeddings\n",
      "    query_result = self._vector_store.query(query, **self._kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/llama_index/vector_stores/lancedb/base.py\", line 563, in query\n",
      "    metadata=metadata,\n",
      "                    ^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/pandas/core/series.py\", line 1121, in __getitem__\n",
      "    return self._get_value(key)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/pandas/core/series.py\", line 1237, in _get_value\n",
      "    loc = self.index.get_loc(label)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/leo4study/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 3812, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 'doc_id'\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“Œ Gradio ì¸í„°í˜ì´ìŠ¤ ì„¤ì •\n",
    "demo = gr.ChatInterface(\n",
    "    answer,\n",
    "    type=\"messages\",\n",
    "    title=\"GraphRAG + Ollama RAG Chatbot\",\n",
    "    description=\"GraphRAGì—ì„œ ìƒì„±í•œ LanceDB ë°ì´í„°ì™€ ì‚¬ìš©ìê°€ ì—…ë¡œë“œí•œ ë¬¸ì„œë¥¼ í™œìš©í•œ Ollama ê¸°ë°˜ RAG Chatbot!\",\n",
    "    textbox=gr.MultimodalTextbox(file_types=[\".pdf\", \".txt\"]),\n",
    "    multimodal=True  # íŒŒì¼ ì—…ë¡œë“œ í—ˆìš©\n",
    ")\n",
    "\n",
    "# ğŸ“Œ ì‹¤í–‰\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leo4study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
