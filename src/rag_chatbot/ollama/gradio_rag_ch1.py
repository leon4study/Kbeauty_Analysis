import gradio as gr
import lancedb
from llama_index.vector_stores.lancedb import LanceDBVectorStore
from llama_index.llms.ollama import Ollama
from llama_index.core import VectorStoreIndex,StorageContext, SimpleDirectoryReader
import os


# ğŸ“Œ GraphRAGì—ì„œ ìƒì„±í•œ LanceDB ë¶ˆëŸ¬ì˜¤ê¸°
db_path = "/Users/jun/GitStudy/Data_4/Data/project5/model/graphrag_t_2/output/lancedb"
db = lancedb.connect(db_path)  # GraphRAGì—ì„œ ì‚¬ìš©í•œ DB ê²½ë¡œ 

# âœ… í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸ í›„ ë¶ˆëŸ¬ì˜¤ê¸°
table_names = db.table_names() 
# âœ… ëª¨ë“  í…Œì´ë¸”ì„ ë¶ˆëŸ¬ì™€ì„œ VectorStoreIndex ìƒì„±
vector_stores = {name: LanceDBVectorStore(table=db.open_table(name)) for name in table_names}
storage_contexts = {name: StorageContext.from_defaults(vector_store=store) for name, store in vector_stores.items()}
indexes = {name: VectorStoreIndex.from_vector_store(store, storage_context=storage_contexts[name]) for name, store in vector_stores.items()}

# ğŸ“Œ Ollama ëª¨ë¸ ì„¤ì • (ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©)
llm = Ollama(model="mistral")  # Ollama ëª¨ë¸ ì„ íƒ (ë¡œì»¬ API) ?!?!?!

# ğŸ“Œ OpenAI API í‚¤ ì—†ì´ ë¡œì»¬ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ì„ë² ë”© ì„¤ì •
from llama_index import ServiceContext

service_context = ServiceContext.from_defaults(embed_model='local')  # ë¡œì»¬ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •
llm.set_service_context(service_context)

# âœ… ì—¬ëŸ¬ í…Œì´ë¸”ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ Query ì—”ì§„ì„ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì €ì¥
query_engines = {name: index.as_query_engine(llm=llm) for name, index in indexes.items()}

# ğŸ“Œ íŒŒì¼ ì—…ë¡œë“œ í›„ ë¬¸ì„œ ì²˜ë¦¬
def process_uploaded_files(files):
    """ì‚¬ìš©ìê°€ ì—…ë¡œë“œí•œ íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ LlamaIndexì— ì¶”ê°€"""
    if not files:
        return None  # íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¬´ì‹œ

    # íŒŒì¼ ì €ì¥ ê²½ë¡œ
    upload_dir = "uploaded_files"
    os.makedirs(upload_dir, exist_ok=True)

    # ì—…ë¡œë“œëœ íŒŒì¼ ì €ì¥
    file_paths = []
    for file in files:
        file_path = os.path.join(upload_dir, file.name)
        file_paths.append(file_path)
        with open(file_path, "wb") as f:
            f.write(file.read())

    # ğŸ“Œ ìƒˆ ë¬¸ì„œ ì¸ë±ì‹±
    documents = SimpleDirectoryReader(input_files=file_paths).load_data()
    new_index = VectorStoreIndex.from_documents(documents)
    return new_index.as_query_engine(llm=llm)

# ğŸ“Œ ì‚¬ìš©ì ë©”ì‹œì§€ ì²˜ë¦¬
def answer(message, history, files):
    """ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë°›ê³ , ê¸°ì¡´ GraphRAG ë°ì´í„° + ì—…ë¡œë“œëœ ë¬¸ì„œ ë°ì´í„°ë¡œ ë‹µë³€"""
    query_engines = list(query_engines.values())  # ê¸°ì¡´ GraphRAG ë°ì´í„°

    # ì—…ë¡œë“œëœ íŒŒì¼ì´ ìˆì„ ê²½ìš° ìƒˆë¡­ê²Œ ì¸ë±ì‹±í•˜ì—¬ ì¶”ê°€
    new_query_engine = process_uploaded_files(files)
    if new_query_engine:
        query_engines.append(new_query_engine)

    # ëª¨ë“  ì¿¼ë¦¬ ì—”ì§„ì—ì„œ ì§ˆì˜ ìˆ˜í–‰
    responses = [qe.query(message["text"]) for qe in query_engines]
    
    # ğŸ“Œ ì‘ë‹µì„ ì¢…í•©í•˜ì—¬ ë°˜í™˜
    return "\n\n---\n\n".join([str(resp) for resp in responses])

# ğŸ“Œ Gradio ì¸í„°í˜ì´ìŠ¤ ì„¤ì •
demo = gr.ChatInterface(
    answer,
    type="messages",
    title="GraphRAG + Ollama RAG Chatbot",
    description="GraphRAGì—ì„œ ìƒì„±í•œ LanceDB ë°ì´í„°ì™€ ì‚¬ìš©ìê°€ ì—…ë¡œë“œí•œ ë¬¸ì„œë¥¼ í™œìš©í•œ Ollama ê¸°ë°˜ RAG Chatbot!",
    textbox=gr.MultimodalTextbox(file_types=[".pdf", ".txt"]),
    multimodal=True  # íŒŒì¼ ì—…ë¡œë“œ í—ˆìš©
)

# ğŸ“Œ ì‹¤í–‰
demo.launch()
